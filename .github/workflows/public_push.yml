name: Push Scraped Data to Public Repo
on:
  schedule:
    - cron: "0 4 * * *" # Runs every day at 4 AM
  workflow_dispatch: # Allows manual runs

jobs:
  scrape-and-push:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Private Repo
        uses: actions/checkout@v2
        with:
          repository: "5ademni/job-scraper"
          token: ${{ secrets.GH_PAT }} # A GitHub PAT that has access to your private repo

      - name: Run Scraper
        run: python main.py

      - name: Commit Scraped Data
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add .
          git commit -m "Update scraped data"

      - name: Push to Private Repo
        run: git push

      - name: Checkout Public Repo
        uses: actions/checkout@v2
        with:
          repository: "5ademni/job-dataset"

      - name: Copy Data to Public Repo
        run: cp -r ../job-scraper/harvest/know_base/* . # Copies all files and subfolders

      - name: Commit and Push to Public Repo
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add .
          git commit -m "Update public data"
          git push
